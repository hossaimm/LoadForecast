---
title: 'BAC2019: Electric Load Forecasting'
author: "JMohammed Hossain| original code from IA650 group project"
date: "created 03/07/2019 | completed 00/00/2019"
output:
  html_document:
    highlight: tango
    theme: cerulean
    toc: yes
    toc_depth: 1
  pdf_document:
    toc: yes
    toc_depth: '1'
---

# Project Introduction

#### Project Description

_Project 2_ in _IA650_ involves analyzing electric load across the state of New York. New York Independent System Operators (NYISO) manage the flow of reliable electricity across New York. NYISO is responsible for cost-effectively matching offers from energy producers with consumer utility demand to supply electric power for the state. NYISO also oversees the delivery of power from generators to the utility companies.\  
* [Learn more](https://www.nyiso.com/what-we-do)\  

Electric load is forecasted and monitored hourly, across eleven NYISO zone areas. The forecasted load values are highly utilized in NYISO's day to day operation. Such as, balancing power, demand response etc.\  
* [NYISO Zonal Map](http://nyarea.org/wp-content/uploads/NY-Electricity-Marketplace-IB.png)\  

The results provided in this report will be reproduceable to an analyst evaluating or furthering the report findings. Each assumption made and step taken in this report will be carefully described. When applicable, relative web links for additional readings and supporting points will be provided. All datasets, code, and logic will be provided and thoroughly explained. All answers provided will follow the [CRISP-DM](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining) framework. The final report will be generated using `RStudio` via editing an .RMD file, and output to an .HTML file via knitting.\  

All supporting files will be provided in this project submission, including knitted .html, rmd code, .xlsx files used, etc.\  

#### Project configuration setttings

The project report will start by performing a few housekeeping items:\  
* Loading libraries used\  
* Aligning various configuration settings\  

```{r package_load, message = FALSE, warning = FALSE}
# Libraries to be used in the project report
library(broom)
library(chron)
library(corrplot)
library(e1071)
library(forecast)
library(fpp2)
library(ggpubr)
library(ggthemes)
library(janitor)
library(kableExtra)
library(knitr)
library(lubridate)
library(plotly)
library(psych)
library(qrsvm)
library(RColorBrewer)
library(readxl)
library(skimr)
library(stats)
library(summarytools)
library(tibbletime)
library(tidyverse)
library(vcd)
```

```{r, include = FALSE}
# Configuration settings for project report
cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
options(scipen = 4)
theme_set(theme_pubr())
```

#### User-Defined functions

In this section of the introduction, one user defined function is declared.

* `modeUDF` _supporting link_ | [modeUDF credit](https://stackoverflow.com/questions/2547402/is-there-a-built-in-function-for-finding-the-mode)\  
    + _Purpose_: to determine mode value of a factor variable\  

```{r}
# Declare modeUDF
modeUDF <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]}
```

***

# Business Understanding

In this section of the project report, a sense of business understanding will be developed and explained with respect to the problem at hand with electric load forecasting. The business understanding framework is an important exercise because it helps to align the analyst with background information and real-world semantics with regard to the data frame(s) being analyzed.  

#### Determine Business Objectives

In the electricity distribution market, pricing decisions for customer energy consumption are influenced by many factors. Some examples of influential factors to pricing decisions are the cost of energy distribution, fuel, and power plant operation. The United States federally allows each state legislature to govern if customer electricity consumption costs are to be regulated, aside from all other costs associated with the process to the business.

The predicted electric load amount is a crucial aspect of the electricity pricing model; it is one of the main factors for pricing decisions. Load forecasting is at the core of nearly all decisions made in energy markets, stemming from the strong influence on the pricing model. The electricity market follows this behavior. Considering that pricing decisions are so important to business health, it would prove helpful to better understand how the current electric load forecasting model is performing, and explore if it can be improved upon. 

Accurate load forecasting can help to better refine many business processes, such as contract evaluation, energy purchasing and generation, load switching, and infrastructure development. Under-estimation by a supplier in an electric load forecast may lead to higher operational costs, due to quick start units cost more for generating power. Over-estimation by a supplier in an electric load forecast may lead to un-economic operation of large generation units, leaving the company with un-used purchased electric energy. It has been discovered that a load forecast error of 1% in terms of Mean Absolute Percentage Error (MAPE) can translate into several hundred thousand dollars for a utilityâ€™s financial bottom line. This statistic emphasizes that businesses in the energy market need to have accurate load forecasting.

This report will focus on the electricity loads in the state of New York, which is a state with regulated electricity pricing enforced. Electricity market decisions are made via a collection of independent system operators. Our study focuses on New York Independent System Operator (NYISO) load data available at the data center weblink provided in the data understanding section of this report. NYISOs are currently performing point forecasting methods to analyze load forecast usig Regression and artificial neural network. We will use different approach to do the load forecasting and compare the results with NYISO. 

_supporting link_ | [Factors Affecting Electricity Prices](https://www.eia.gov/energyexplained/index.php?page=electricity_factors_affecting_prices)

#### Assess Situation

##### Identification of Stakeholders

The stakeholders of the process are:\    
* each Independent System Operator (ISO)\  
    + This report focuses on NYISO\  
* Local distribution companies (LDC)\  
    + eg. National Grid\  
* Electric generation stations\  
* Customers of electric distribution companies\  

##### Interests of the Identified Stakeholders

Ensuring a forecast is accurate, as to align on spending and resource management.\  
* The more accurate the forecast, the less money an ISO will lose in buying too much energy and not being able to use it, or not buying enough energy and having to go back and buy more at a higher price.\  
* For the LDC's, a more accurate model means they will be able to better predict the price for the customer and better align their business operations, ensuring higher profit.\  
* Generation stations benefit from a more accurate load prediction model as well, since they will be able to better predict what electric generation stations are required to be running.\  
* The customers will benefit from a better model as the price paid for electricity will be more consistent, avoiding higher prices with understimation in load forecast.\  

#### Data Mining Goals

The purpose of the data mining methods used in this report are to resimulate the forecasting of electric load values across the NYISO. After running new forecasting models, the report will check performance of the forecasted values against the original forecasts used by NYISO. 

#### Project Plan

As suggested by Hong(2010), this project report will use three years of actual electric load data (Jan 1 2015 to Dec 31 2017) as training data to build a model that calculates the electric load forecast for the next day. Following the model generation, the report will use a rolling forecast on the actual data of a time period to recalculate the model and forecast the next day for the year of 2018 (Jan 1st 2018 to Nov 15th 2018).  

This report will use two forecasting methods to produce possible electric load values.\  
1. Linear Regression Model\  
2. ARIMA Model\  

***

# Data Understanding

In this section of the project report, the data understanding topics from the CRISP-DM framework are explored.  

#### Collect Initial Data

Three sets of raw data were collected from the NYISO data load website:\  
* Actual Electric Load data\  
    + Under _Actual Load_ section, select _Real-Time_ dropdown\  
    + Select option to generate a custom report\  
    + Select all available Zones\  
    + Select start date of _01/01/2015_, end date of _11/15/2018_\  
    + Select to download in .CSV format\  
* Forecast Electric Load data\  
    + Under _Load Forecast_ section, select _NY Load Forecast_ dropdown\  
    + Select option to generate a custom report\  
    + Select all available Zones\  
    + Select start date of _01/01/2015_, end date of _11/15/2018_\  
    + Select to download in .CSV format\  
* Weather data\  
    + Under _Load Forecast_ section, select _Load Forecast Weather Data_ dropdown\  
    + Select option to view archive\  
    + Select _Archived Files (zip format)_ for months of 01-2015 thru 11-2018\  
    + Unzip each monthly archive and store in all .CSV files in a central folder\  

_data download link_ | [NYISO Data Center](https://www.nyiso.com/load-data)

Each set of raw data downloaded will be initially stored as a collection of .CSV files. There is a process to merge multiple .CSV files into a central .CSV file, using either the command line (Windows/Linux) or the terminal (MacOS). Here is documentation on how to convert the .CSV files using both platforms:\  
* Windows/Linux | [How to merge multiple .CSV files using Windows/Linux](http://www.tomnash.eu/how-to-combine-multiple-csv-files-into-one-using-cmd/)\  
* MacOS | [How to merge multiple .CSV files using MacOS](https://eikhart.com/blog/merge-csv-files-with-mac-os-x-terminal)\  

Once data are in three combined .CSV files, the files need to be merged and converted to .XLSX files. The two merged load data .CSV files (actual and forecasted load) are converted into a single .CSV and then saved as an .XLSX, `nys_electric_load.xlsx`. The merged weather data is saved as an .XLSX, `nys_weather_data.xlsx`.\  
* Each of the conversions from .CSV to .XLSX is done using Microsoft Excel\  
* _Please Note_: The column header names are changed to be more "code friendly"\  
    + See what the column headers are in the next _Describe Data_ section, and change the headers in the raw .XLSX sheets to match\  

```{r}
# Upload raw electric load and forecast data from .xlsx file
load.init <- read_excel("nys_electric_load.xlsx")

# Upload raw weather forecast data from .xlsx file
wthr.init <- read_excel("nys_weather_data.xlsx")
```

#### Describe Data

After the raw data frames are loaded, the structure of each data frame is inspected to gain an understanding of the various types of variables amongst the raw data.  

```{r}
# Structure of raw load data frame
str(load.init)
```

__Initial Load Data__ | `load.init` data frame described, with comments about significant observations\  
* `date_hour`, POSIXct | Time stamp of each observation in yyyy-mm-dd hh:mm:ss format\  
* `gmt_hour`, num | Greenwich Mean Time (GMT) of each observation\  
    + This data attribute is redundant, as it is not necessary to encode the hourly time stamp in two attributes\  
* `day`, chr | Day of week that the observation is made\  
    + This chr variable will need to be formatted to a factor variable\  
* `zone`, chr | NYISO zone in which the observation is made, by name\  
    + This chr variable will need to be formatted to a factor variable\  
* `zone_id`, num | NYSIO zone in which the observation is made, by ID\  
    + This data attribute is redundant, as it is encoding the same information as `zone`\  
* `load_actual`, num | Actual hourly electric load value observed\  
* `load_forecast`, num | Forecasted hourly electric load value made\  
* `rmse`, num | Root Mean Squared Error (RMSE) measure of the observation\  
    + The RMSE value was calculated from rows existing in the raw data\  
* `mape`, num | Mean Absolute Percentage Error (MAPE) measure of the observation\  
    + The MAPE value was calculated from rows existing in the raw data\  

The formulas for MAPE and RMSE are well-known, but can be explicitly found in Yang et al. (2018).  

```{r}
# Structure of raw weather data frame
str(wthr.init)
```

__Initial Weather Data__ | `wthr.init` data frame described, with comments about significant observations\  
* `forecast_date`, POSIXct | Date in which a forecast was made for the observation\  
    + This variable is not important to this analysis\  
    + This project is more focused on the true date of the observation\  
* `date`, POSIXct | Date of the weather information for the observation\  
* `station_id`, chr | Weather station at which the observation was made\  
    + Each NYISO zone contains potentially many weather stations\  
    + The weather station was used to determine which NYISO zone the observation pertains to\  
* `max_temp`, num | Maximum temperature value (in degrees farenheight) achieved in the observation date\  
* `min_temp`, num | Minimum temperature value (in degrees farenheight) achieved in the observation date\  
* `temp_diff`, num | Numerical value of the difference in maximum and minimum temperatures achieved in the observation date\  
    + This value was calculated based on the formula `max_temp - min_temp`\  
* `max_wet_bulb`, num | Maximum humidity level achived in the observation date\  
* `min_wet_bulb`, num | Minimum humidity level achived in the observation date\  
* `diff_wet_bulb`, num | Difference in humidity levels achived in the observation date\  
    + This value was calculated based on the formula `max_wet_bulb - min_wet_bulb`\  
* `zone`, chr | NYISO zone in which the observation is made, by name\  
    + This chr variable will need to be formatted to a factor variable\  

#### Verify Data Quality

Data quality can be measured in various ways. However, this project report looks at data quality in terms of data accuracy, completeness (missing data), consistency (outliers), and uniqueness (duplicate data). The following sub sections evaluate data quality in this fashion.  
* _Note_: in terms of data accuracy, it is assumed that the data utilized is highly accurate  
    + The NYISO groups provide the most accurate data available with respect to electric load  
    + The airports across the state of NY provide the most accurate weather data available  

##### Explore Missing Data

After initial inspection of the load data that was downloaded for the intended time frame, there were no hourly observations that were missing.  

After initial inspection of the weather data that was downloaded for the intended time frame, there was one date with no recorded observations. The date of 09/10/2017 was found to have missing data values.  
* In order to move past this, the weather values from the next date of 09/11/2017 are copied into the date of 09/10/2017.   
* This step is done to ensure that each date in the time frame had values that were present.  
    + Statistical models described below rely on a complete time series in order to function the best they can  

##### Explore Duplicate Data

After inspection of the initial load and weather data frames, there were no duplicated data values.  
* This was determined because there was a single observation for each unit of time intended to be used by the analyses  

##### Explore for Outlier Data

Outliers are not considered in this report, as all load value is important to NYISO in analyses. Every load needs to be taken care of, even the high and low points.    

###### Note on Determinations Made from Data Quality Exploration

The explorations noted directly above were an attempt to address data quality concerns. However, not all of these explorations were intended to correct mistakes made in the data. The exploration of outlier data was not intended to correct a mistake made in raw data frames downloaded. However, the missing data points that were found are considered to be a mistake, as theoretically all data points should be available. Also, if duplicate data points were found, that would also be a mistake, but on the fault of the analyst for not preprocessing the data correctly.  

*** 

# Data Preparation

In this step of the project report, steps will be taken to prepare the raw data frame to be better suited for analysis. Not all attributes in the initial data frame are necessary to analyze. There were some deficiencies in the way the data was loaded (as described in the _Data Understanding_ section) that need to be fixed. A final set of data frames will also need to be made in order to align the data to the programming performed.\  
* There will be multiple data frames created:\  
    + To transform various parts of each raw data frame\  
    + To create various data frames for different analytic approaches\  

#### Select Data

The variables that are not important to analysis were outlined in the Data Understanding section. They will not be pulled out of the data frame here, rather handled in the Construct Data section through the use of dplyr chains. This section of the report will not clean out the unnecessary data attributes from the raw `load.init` and `wthr.init` data frames.  

#### Clean Data

This section of the project report will ensure the data frames are cleaned and optimized. The initial part of the cleaning phase is to ensure there is no white space or differences in letter casing amongst the data values of the data frame being used to run analyses.\  
* Initial names based on time interval nature of raw data.\  

```{r, message = FALSE, warning = FALSE}
# Load data - Hourly data frame creation
load.hour <- load.init %>% mutate_if(is_character, ~str_to_upper(.) %>% str_trim())

# Weahter data - Daily data frame creation
wthr.day <- wthr.init %>% mutate_if(is_character, ~str_to_upper(.) %>% str_trim())
```

Identified deficiencies in the variable types are now handled.

```{r}
# Load data frame manipulation
load.hour$gmt_hour <- as.factor(load.hour$gmt_hour)
load.hour$day <- as.factor(load.hour$day)
load.hour$zone <- as.factor(load.hour$zone)
load.hour$zone_id <- as.factor(load.hour$zone_id)

# Weather data frame manipulation
wthr.day$station_id <- as.factor(wthr.day$station_id)
wthr.day$zone <- as.factor(wthr.day$zone)
```

#### Construct Data

This section of the project report will construct and manipulate various data frames that will be used in analyses to follow.

There are multiple weather stations per NYISO zone in the weather data. Each weather station is located inside of an NYISO zone. The weather station data values are aggregated based on the NYISO zone.

```{r}
# Aggreage weather data by NYISO zone
wthr.day <- wthr.day %>%
  mutate(date = (date(date))) %>%
  group_by(date, zone) %>%
  summarize(max_temp = mean(max_temp), min_temp = mean(min_temp),
            temp_diff = mean(temp_diff), max_wet_bulb = mean(max_wet_bulb),
            min_wet_bulb = mean(min_wet_bulb), diff_wet_bulb = mean(diff_wet_bulb))
```

Since the `load.hour` and `wthr.day` are divided up by different time units (hours vs days), hourly electric load data must be aggregated into daily electric load data.  

```{r}
# Load data - Daily data frame creation
load.day <- load.hour %>%
  mutate(date = (date(date_hour))) %>%
  group_by(date, zone) %>%
  summarize(day = as.factor(modeUDF(day)), load_actual = sum(load_actual), load_forecast = sum(load_forecast))
```

After creating `load.day`, it can be observed that `wthr.day` has more days contained in it than `load.day` does. In order to align these data frames, the time frames must be equal.\  
* Remove some of the values in the `wthr.day` data frame\  

```{r}
# Weather data - Slice daily frame to match time frame of daily load
wthr.dayslice <- wthr.day[12:15576,] # 1/1/15 - 11/15/18
```

It would also be helpful to have an aggregated daily count of the load and weather values, across all zones.\  
* This would represent NY-state wide data\  

```{r}
# Load data - Aggregate daily frame creation for NYS
load.dayagg <- load.day %>%
  group_by(date) %>%
  summarize(day = as.factor(modeUDF(day)), load_actual = sum(load_actual), load_forecast = sum(load_forecast))

# Weather data - Aggregate daily frame creation for NYS
wthr.dayagg <- wthr.day %>%
  group_by(date) %>%
  summarize(max_temp = mean(max_temp), min_temp = mean(min_temp),
            max_wet_bulb = mean(max_wet_bulb), min_wet_bulb = mean(min_wet_bulb)) %>%
  slice(2:1416) # 1/1/15 - 11/15/18
```

#### Integrate Data

Now that both the aggregated and daily zonal weather and load data frames are within the same time frame, merge them to make analysis purposes simpler.\  
* There will be a new data frame created for the daily zonal data\  
* There will be a new data frame created for the aggregated data\  

```{r}
# Merge both data frames
df.day <- merge(x = load.day, y = wthr.dayslice, by = c("date", "zone"), all = TRUE)
df.dayagg <- merge(x = load.dayagg, y = wthr.dayagg, by = c("date"), all = TRUE)
```


#### Format Data

It can be observed in `df.day` and `df.dayagg` that the `date` attributes are in _Date_ format. In order to better pursue time series analyses, these data frames are copied into another data frame so the `date` attributes can be changed to _POSIXct_ format.  

```{r}
# Create data frames more suiteable for time series
ts.day <- df.day
ts.day$date <- as.POSIXct(ts.day$date)
ts.dayagg <- df.dayagg
ts.dayagg$date <- as.POSIXct(ts.dayagg$date)

# need to make new slice data frames to use with linear models utilizing diffs
df.dayagg.slice <- df.dayagg %>% slice(1:1415) # 1/1/15 - 11/15/18
ts.dayagg.slice <- ts.dayagg %>% slice(2:1416) # 1/1/15 - 11/14/18
```

#### Explore Data

After data has been cleaned, merged, and formatted, it can now be explored. Basic statistical figures were generated to gain a sense of what the data frames look like. Gaining exploratory knowledge around a data frame aids in data understanding efforts, as well as makes the analyst better equipped to perform model generation.  

In order to run some statistical tests, it would be helpful to remove all data variables that are non-numeric.  

```{r}
# Remove all data attributes that are non-numeric from ts.dayagg
num.dayagg <- ts.dayagg %>% select(-c(date, day, load_forecast))
```

1. Feature 'Normalized Z' scaling of the "actual load" 

```{r}
#dfNormZ <- as.data.frame( scale(df[2]))

dfNormZ <- as.matrix( scale(ts.day["load_actual"]))
#ts.day$Scaled <- ( scale(ts.day["load_actual"]))
```

Ploting the scaled data 

```{r}
ggplot(data = ts.day,
       aes(x = max_temp, y = dfNormZ)) +
  #geom_jitter(width = 0.15) +
  guides(fill = TRUE) +
  stat_smooth(aes(group = 1), method = 'lm', formula = y ~ poly(x, 2, raw=TRUE)) +
  ggtitle(label = "Relationship Between Max Temp and Normalized 'Z' Actual Load",
          subtitle = "Color Encodes Various Zones") +
  xlab("Maximum Temperature (in Degrees Farenheight)") + ylab("Normalized 'Z' Actual Load")
```

2. max min feature scaling 

############work on this 
```{r}
#dfNormZ <- as.data.frame( scale(df[2]))
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}


dfNormZ <-as.matrix(lapply(ts.day["load_actual"], normalize))
#ts.day$scaled <- dfNormZ
```

Ploting the scaled data 

```{r}
ggplot(data = ts.day,
       aes(x = max_temp, y = dfNormZ, color = zone)) +
  geom_jitter(width = 0.15) +
  guides(fill = TRUE) +
  #stat_smooth(aes(group = 1), method = 'lm', formula = y ~ poly(x, 2, raw=TRUE)) +
  ggtitle(label = "Relationship Between Max Temp and Normalized 'Z' Actual Load",
          subtitle = "Color Encodes Various Zones") +
  xlab("Maximum Temperature (in Degrees Farenheight)") + ylab("Normalized 'Z' Actual Load")
```




3. Feature 'Unit' scaling of the "actual load" 

```{r}
#need to fix this code 
scalar1 <- function(x) 
  {x / sqrt(sum(x^2))}


Unit <- as.data.frame( lapply(ts.day["load_actual"], scalar1))
ts.day$scale1 <- Unit["load_actual"]

```

Ploting the data. 

```{r}
ggplot(data = ts.day,
       aes(x = max_temp, y = Scaled, color = zone)) +
  geom_jitter(width = 0.15) +
  guides(fill = TRUE) +
  #stat_smooth(aes(group = 1), method = 'lm', formula = y ~ poly(x, 2, raw=TRUE)) +
  ggtitle(label = "Relationship Between Max Temp and Unit scaled Actual Load",
          subtitle = "Color Encodes Various Zones") +
  xlab("Maximum Temperature (in Degrees Farenheight)") + ylab("Unit scaling Actual Load")
```



It can be observed from the above two graphs that the maximum and minimum values for `max_temp` and `min_temp` are following a consistent pattern over time.  

```{r}
ggplot(data = ts.day,
       aes(x = max_temp, y = load_actual, color = zone)) +
  geom_jitter(width = 0.15) +
  guides(fill = TRUE) +
  stat_smooth(aes(group = 1), method = 'lm', formula = y ~ poly(x, 2, raw=TRUE)) +
  ggtitle(label = "Relationship Between Max Temp and Actual Load MWh",
          subtitle = "Color Encodes Various Zones") +
  xlab("Maximum Temperature (in Degrees Farenheight)") + ylab("Actual Load (in MWh)")
```


Plotting function for both max and min temperature. 

```{r}
#plotting function for max and min temp VS actual load 
LoadPlot <- function(zoneName){
  #filtering for single zone
  load.1zone <- ts.day %>% filter(zone == zoneName)
  
  #plotting data #create plot for MAX temp
  p1<-
  ggplot(data = load.1zone, aes(x = max_temp, y = load_actual)) +
  geom_jitter(width = 0.15) +
  guides(fill = TRUE) +
  stat_smooth(aes(group = 1), method = 'lm', formula = y ~ poly(x, 2, raw=TRUE)) +
  ggtitle(label = zoneName) +
  xlab("Max Temp") + ylab("Actual Load(MWh) ")
  print (p1)
  #fitting lm with 2nd degree poly feature 
  oneZone_lm1 <- lm(formula = load_actual ~ poly(max_temp, 2, raw=TRUE), data = load.1zone)
  print (oneZone_lm1)

  #create plot for MIN temp
  p2<-
  ggplot(data = load.1zone, aes(x = min_temp, y = load_actual)) +
  geom_jitter(width = 0.15) +
  guides(fill = TRUE) +
  stat_smooth(aes(group = 1), method = 'lm', formula = y ~ poly(x, 2, raw=TRUE)) +
  ggtitle(label = zoneName) +
  xlab("Min Temp") + ylab("Actual Load(MWh) ")
  
  print (p2)
  #fitting lm with 2nd degree poly feature 
  oneZone_lm2 <- lm(formula = load_actual ~ poly(min_temp, 2, raw=TRUE), data = load.1zone)
  print (oneZone_lm2)
return()
}

```

![NYISO zones.](C:\Users\rivon\Google Drive\8_spring2019\Project\Zones3.png)


max and min plot for zones.
```{r}
#just put zoneName in the function LoadPlot
LoadPlot('WEST')
LoadPlot('GENESE')
LoadPlot('CENTRL')
LoadPlot('NORTH')
LoadPlot('MHK VL')
LoadPlot('CAPITL')
LoadPlot('HUD VL')
LoadPlot('MILLWD')
LoadPlot('DUNWOD')
LoadPlot('N.Y.C.')
LoadPlot('LONGIL')

```
